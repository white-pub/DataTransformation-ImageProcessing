{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52fe37a-1b7a-4baf-9106-cd29865bdf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "df1\n",
      "\n",
      "   0    1    2    3    4    5    6    7    8    9    ...  490  491  492  493  \\\n",
      "0    1    1    1    1    1    1    1    1    1    1  ...    1    1    1    1   \n",
      "1    1    1    1    1    1    1    1    1    1    1  ...    1    1    1    1   \n",
      "2    1    1    1    1    1    1    1    1    1    1  ...    1    1    1    1   \n",
      "3    1    1    1    1    1    1    1    1    1    1  ...    1    1    1    1   \n",
      "4    1    1    1    1    1    1    1    1    1    1  ...    1    1    1    1   \n",
      "\n",
      "   494  495  496  497  498  499  \n",
      "0    1    1    1    1    1    1  \n",
      "1    1    1    1    1    1    1  \n",
      "2    1    1    1    1    1    1  \n",
      "3    1    1    1    1    1    1  \n",
      "4    1    1    1    1    1    1  \n",
      "\n",
      "[5 rows x 500 columns]\n",
      "----\n",
      "\n",
      "df2\n",
      "\n",
      "   0    1    2    3    4    5    6    7    8    9    ...  592  593  594  595  \\\n",
      "0  250  250    3    3    3    3    3    3    3    3  ...    3    3    3    3   \n",
      "1  250  250    3    3    3    3    3    3    3    3  ...    3    3    3    3   \n",
      "2  250  250    3    3    3    3    3    3    3    3  ...    3    3    3    3   \n",
      "3  250  250    3    3    3    3    3    3    3    3  ...    3    3    3    3   \n",
      "4  250  250    3    3    3    3    3    3    3    3  ...    3    3    3    3   \n",
      "\n",
      "   596  597  598  599  600  601  \n",
      "0    3    3    3    3    3    3  \n",
      "1    3    3    3    3    3    3  \n",
      "2    3    3    3    3    3    3  \n",
      "3    3    3    3    3    3    3  \n",
      "4    3    3    3    3    3    3  \n",
      "\n",
      "[5 rows x 602 columns]\n",
      "----\n",
      "\n",
      "df3\n",
      "\n",
      "   0     1     2     3     4     5     6     7     8     9     ...  1146  \\\n",
      "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "3     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "4    32    32    32    32    32    32    32    32    32    32  ...    32   \n",
      "\n",
      "   1147  1148  1149  1150  1151  1152  1153  1154  1155  \n",
      "0     0     0     0     0     0     0     0     0     0  \n",
      "1     0     0     0     0     0     0     0     0     0  \n",
      "2     0     0     0     0     0     0     0     0     0  \n",
      "3     0     0     0     0     0     0     0     0     0  \n",
      "4    32    32    32    32    32     0     0     0     0  \n",
      "\n",
      "[5 rows x 1156 columns]\n",
      "----\n",
      "\n",
      "df4\n",
      "\n",
      "    0     1     2     3     4     5     6     7     8     9    ...  490  491  \\\n",
      "0  1000  1000  1000  1000  1000  1000  1000  1000  1000  1000  ...   12   12   \n",
      "1  1000  1000  1000  1000  1000  1000  1000  1000  1000  1000  ...   12   12   \n",
      "2  1000  1000  1000  1000  1000  1000  1000  1000  1000  1000  ...   12   12   \n",
      "3  1000  1000  1000  1000  1000  1000  1000  1000  1000  1000  ...   12   12   \n",
      "4  1000  1000  1000  1000  1000  1000  1000  1000  1000  1000  ...   12   12   \n",
      "\n",
      "   492  493  494  495  496  497  498  499  \n",
      "0   12   12   12   12   12   12   12   12  \n",
      "1   12   12   12   12   12   12   12   12  \n",
      "2   12   12   12   12   12   12   12   12  \n",
      "3   12   12   12   12   12   12   12   12  \n",
      "4   12   12   12   12   12   12   12   12  \n",
      "\n",
      "[5 rows x 500 columns]\n",
      "----\n",
      "\n",
      "df5\n",
      "\n",
      "   0    1    2    3    4    5    6    7    8    9    ...  491  492  493  494  \\\n",
      "0    1    1    1    1    1    1    1    1    1    1  ...    1    1    1    1   \n",
      "1    1    1    1    1    1    1    1    1    1    1  ...    1    1    1    1   \n",
      "2    1    1    1    1    1    1    1    1    1    1  ...    1    1    1    1   \n",
      "3    1    1    1    1    1    1    1    1    1    1  ...    1    1    1    1   \n",
      "4    1    1    1    1    1    1    1    1    1    1  ...    1    1    1    1   \n",
      "\n",
      "   495  496  497  498  499  500  \n",
      "0    1    1    1    1    1    0  \n",
      "1    1    1    1    1    1    0  \n",
      "2    1    1    1    1    1    0  \n",
      "3    1    1    1    1    1    0  \n",
      "4    1    1    1    1    1    0  \n",
      "\n",
      "[5 rows x 501 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import needed packages\n",
    "import pandas as pd\n",
    "\n",
    "# Load the files. Specifile file path since the .data are in a subfolder.\n",
    "df1 = pd.read_csv('MysteryData/Mystery1.data', header=None)\n",
    "print(\"----\\n\\ndf1\\n\")\n",
    "print(df1.head())\n",
    "\n",
    "# Repeat for the other files\n",
    "df2 = pd.read_csv('MysteryData/Mystery2.data', header=None)\n",
    "print(\"----\\n\\ndf2\\n\")\n",
    "print(df2.head())\n",
    "df3 = pd.read_csv('MysteryData/Mystery3.data', header=None)\n",
    "print(\"----\\n\\ndf3\\n\")\n",
    "print(df3.head())\n",
    "df4 = pd.read_csv('MysteryData/Mystery4.data', header=None)\n",
    "print(\"----\\n\\ndf4\\n\")\n",
    "print(df4.head())\n",
    "df5 = pd.read_csv('MysteryData/Mystery5.data', header=None)\n",
    "print(\"----\\n\\ndf5\\n\")\n",
    "print(df5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a83a191-0bb8-4846-b186-6bcc0de2de3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "df1\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Columns: 500 entries, 0 to 499\n",
      "dtypes: int64(500)\n",
      "memory usage: 1.9 MB\n",
      "----\n",
      "\n",
      "df2\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 762 entries, 0 to 761\n",
      "Columns: 602 entries, 0 to 601\n",
      "dtypes: int64(602)\n",
      "memory usage: 3.5 MB\n",
      "----\n",
      "\n",
      "df3\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1472 entries, 0 to 1471\n",
      "Columns: 1156 entries, 0 to 1155\n",
      "dtypes: int64(1156)\n",
      "memory usage: 13.0 MB\n",
      "----\n",
      "\n",
      "df4\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Columns: 500 entries, 0 to 499\n",
      "dtypes: int64(500)\n",
      "memory usage: 1.9 MB\n",
      "----\n",
      "\n",
      "df5\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Columns: 501 entries, 0 to 500\n",
      "dtypes: int64(501)\n",
      "memory usage: 1.9 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"----\\n\\ndf1\\n\")\n",
    "df1.info()\n",
    "\n",
    "print(\"----\\n\\ndf2\\n\")\n",
    "df2.info()\n",
    "\n",
    "print(\"----\\n\\ndf3\\n\")\n",
    "df3.info()\n",
    "\n",
    "print(\"----\\n\\ndf4\\n\")\n",
    "df4.info()\n",
    "\n",
    "print(\"----\\n\\ndf5\\n\")\n",
    "df5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2574cef8-e982-4474-9d46-d03a4924daa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3734 entries, 0 to 3733\n",
      "Columns: 1156 entries, 0 to 1155\n",
      "dtypes: float64(656), int64(500)\n",
      "memory usage: 32.9 MB\n",
      "None\n",
      "   0     1     2     3     4     5     6     7     8     9     ...  1146  \\\n",
      "0     1     1     1     1     1     1     1     1     1     1  ...   NaN   \n",
      "1     1     1     1     1     1     1     1     1     1     1  ...   NaN   \n",
      "2     1     1     1     1     1     1     1     1     1     1  ...   NaN   \n",
      "3     1     1     1     1     1     1     1     1     1     1  ...   NaN   \n",
      "4     1     1     1     1     1     1     1     1     1     1  ...   NaN   \n",
      "\n",
      "   1147  1148  1149  1150  1151  1152  1153  1154  1155  \n",
      "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "2   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "3   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "4   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "\n",
      "[5 rows x 1156 columns]\n"
     ]
    }
   ],
   "source": [
    "# The files have the same structure, so assemble them\n",
    "df_combined = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)\n",
    "\n",
    "# View the combined DataFrame\n",
    "print(df_combined.info())\n",
    "print(df_combined.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a413b494-f9c9-4509-b705-065eb9fcab5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "\n",
      "df1\n",
      "\n",
      "(500, 500)\n",
      "----\n",
      "\n",
      "df2\n",
      "\n",
      "(762, 602)\n",
      "----\n",
      "\n",
      "df3\n",
      "\n",
      "(1472, 1156)\n",
      "----\n",
      "\n",
      "df4\n",
      "\n",
      "(500, 500)\n",
      "----\n",
      "\n",
      "df5\n",
      "\n",
      "(500, 501)\n"
     ]
    }
   ],
   "source": [
    "print(\"----\\n\\ndf1\\n\")\n",
    "print(df1.shape)  \n",
    "print(\"----\\n\\ndf2\\n\")\n",
    "print(df2.shape)  \n",
    "print(\"----\\n\\ndf3\\n\")# After further exploration, they share similar properties, but should not be combines\n",
    "print(df3.shape)  \n",
    "print(\"----\\n\\ndf4\\n\")\n",
    "print(df4.shape)  \n",
    "print(\"----\\n\\ndf5\\n\")\n",
    "print(df5.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6d4f6-2e2e-40e0-be48-8d8fa76ab72d",
   "metadata": {},
   "source": [
    "After further exploration, the five files share similar properties, but should not be combines\n",
    "# The great reveal about the mystery data\n",
    "The professor revealed that the five files are one picture each.\n",
    "In each picture, a value represents a pixel and a RGB color code.\n",
    "Our goal is to transform the .data files into picturees, and fine-tune the color."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2848f96d-d2a5-4574-88b3-af062f7f8d18",
   "metadata": {},
   "source": [
    "## Attemp 1: ask Chat GPT and try the method provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f47ab073-33e7-4e53-82bd-a874e1749482",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string '1,' to float64 at row 0, column 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 1: Load the .data file (adjust depending on the file format)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMysteryData/Mystery1.data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# assuming it's a text file with rows of integers\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Step 2: Normalize data to ensure values fall within the 0-255 range for grayscale\u001b[39;00m\n\u001b[0;32m      8\u001b[0m min_val \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mmin()\n",
      "File \u001b[1;32m~\\CS322 Foundation of Data Science\\Foundation-of-Data-Science-projects\\venv\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:1397\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1395\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1397\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1399\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32m~\\CS322 Foundation of Data Science\\Foundation-of-Data-Science-projects\\venv\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:1036\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m   1033\u001b[0m     data \u001b[38;5;241m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_dtype_via_object_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1036\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_filelike\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimaginary_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimaginary_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiplines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilelike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilelike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbyte_converters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbyte_converters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;66;03m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;66;03m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m     \u001b[38;5;66;03m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filelike:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string '1,' to float64 at row 0, column 1."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1: Load the .data file (adjust depending on the file format)\n",
    "data = np.loadtxt('MysteryData/Mystery1.data')  # assuming it's a text file with rows of integers\n",
    "\n",
    "# Step 2: Normalize data to ensure values fall within the 0-255 range for grayscale\n",
    "min_val = data.min()\n",
    "max_val = data.max()\n",
    "\n",
    "# Normalize data to 0-255\n",
    "normalized_data = (data - min_val) / (max_val - min_val) * 255\n",
    "\n",
    "# Step 3: Convert to an image\n",
    "# No need to convert to RGB triples, just create a grayscale image using the 2D array\n",
    "image = Image.fromarray(np.uint8(normalized_data), 'L')  # 'L' mode for grayscale image\n",
    "\n",
    "# Step 4: Save as PNG or JPG\n",
    "image.save('output_image.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d629b569-3302-4a0f-be81-6e1d8c8b3219",
   "metadata": {},
   "source": [
    "### Key Points\n",
    "* Normalization:\n",
    "    * This ensures that the pixel values are in the correct range for a grayscale image (0â€“255).\n",
    "* Grayscale Image Mode: \n",
    "    * The 'L' mode tells Pillow that you're creating a grayscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1d8c3-17be-43ce-8168-7ed5e739e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1: Load the .data file with comma as the delimiter\n",
    "data = np.loadtxt('MysteryData/Mystery1.data', delimiter=',')  # Specify the delimiter\n",
    "\n",
    "# Step 2: Normalize data to ensure values fall within the 0-255 range for grayscale\n",
    "min_val = data.min()\n",
    "max_val = data.max()\n",
    "\n",
    "# Normalize data to 0-255\n",
    "normalized_data = (data - min_val) / (max_val - min_val) * 255\n",
    "\n",
    "# Step 3: Convert to an image\n",
    "image = Image.fromarray(np.uint8(normalized_data), 'L')  # 'L' mode for grayscale image\n",
    "\n",
    "# Step 4: Save as PNG or JPG\n",
    "image.save('output_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59ff2c-7548-41f4-835b-8498fe7468f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b857b3b-321f-4825-9fe0-c92aadbaffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a text file, without headers or index\n",
    "df1.to_csv('full_data.txt', index=False, header=False)\n",
    "\n",
    "print(\"Data saved to 'full_data.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9442dc-eadf-412a-bb7c-ba6d8a368183",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e01df11-b765-4022-bdcb-0ade861cade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc8931-d8a5-4093-b89d-dae8d8f00269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Flatten the DataFrame to 1D if it's multi-dimensional (to avoid complications)\n",
    "df1_flat = df1.values.flatten()\n",
    "\n",
    "# Plot histogram using seaborn\n",
    "sns.histplot(df1_flat, bins=70, kde=True)  # kde=True adds a smooth curve\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Values in df1')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb60cfa-c3fc-4a7c-a6ad-8e0efcb8121f",
   "metadata": {},
   "source": [
    "#### Handling the Bimodal Distribution\n",
    "For U-shaped or bimodal distributions, normalization might not fully address the concentration of values around two distinct regions (0-100 and 900-1000). In such cases, you might want to:\n",
    "\n",
    "* Use log transformation to compress large values and expand small ones, making the distribution more uniform. This works well when there is a large gap in value ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa4426-8b27-4417-8d44-25bc6f3f4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Apply log transformation (add small constant to avoid log(0))\n",
    "log_transformed_data = np.log1p(df1_flat)  # log1p(x) = log(1 + x) to handle zeros\n",
    "\n",
    "# Plot histogram of log-transformed data\n",
    "sns.histplot(log_transformed_data.flatten(), bins=50, kde=True)\n",
    "plt.title('Log-Transformed Data Distribution')\n",
    "plt.xlabel('Log Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee532ec-d395-4c5c-ba2b-3752938c2159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Normalize data to ensure values fall within the 0-255 range for grayscale\n",
    "min_val = log_transformed_data.min()\n",
    "max_val = log_transformed_data.max()\n",
    "\n",
    "# Normalize data to 0-255\n",
    "normalized_data = (data - min_val) / (max_val - min_val) * 255\n",
    "\n",
    "\n",
    "# Step 3: Convert to an image\n",
    "image = Image.fromarray(np.uint8(normalized_data), 'L')  # 'L' mode for grayscale image\n",
    "\n",
    "# Step 4: Save as PNG or JPG\n",
    "image.save('output_image_log.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5431f-3e86-434d-be8c-ef68a6e1f9ba",
   "metadata": {},
   "source": [
    "#### Result - quite detaailded gray scale image. Just need some coloring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63226c83-9cd2-4644-8d0a-1cd526bbb5b0",
   "metadata": {},
   "source": [
    "# Picture 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f8275c-b05d-4fdd-8c58-74fd53d2c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pic 2\n",
    "# Flatten the DataFrame to 1D if it's multi-dimensional (to avoid complications)\n",
    "df2_flat = df2.values.flatten()\n",
    "\n",
    "# Plot histogram using seaborn\n",
    "sns.histplot(df2_flat, bins=70, kde=True)  # kde=True adds a smooth curve\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Values in df2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd15d1-5982-4b79-9676-9e8befda319b",
   "metadata": {},
   "source": [
    "# Picture 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35d4aa-883d-4649-94e6-e523ae641a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pic 3\n",
    "# Flatten the DataFrame to 1D if it's multi-dimensional (to avoid complications)\n",
    "df3_flat = df3.values.flatten()\n",
    "\n",
    "# Plot histogram using seaborn\n",
    "sns.histplot(df3_flat, bins=70, kde=True)  # kde=True adds a smooth curve\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Values in df3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b4295-b81e-4a5d-8202-d4866f51eab2",
   "metadata": {},
   "source": [
    "# Picture 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51853b28-b7a6-468f-8795-cfd6d4b39991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pic 4\n",
    "# Flatten the DataFrame to 1D if it's multi-dimensional (to avoid complications)\n",
    "df4_flat = df4.values.flatten()\n",
    "\n",
    "# Plot histogram using seaborn\n",
    "sns.histplot(df4_flat, bins=70, kde=True)  # kde=True adds a smooth curve\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Values in df4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6df092-9f2e-44d2-97a6-4c4ad8cc4789",
   "metadata": {},
   "source": [
    "# Picture 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091c87f-4028-49d8-b023-724060de6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pic 5\n",
    "# Flatten the DataFrame to 1D if it's multi-dimensional (to avoid complications)\n",
    "df5_flat = df5.values.flatten()\n",
    "\n",
    "# Plot histogram using seaborn\n",
    "sns.histplot(df5_flat, bins=70, kde=True)  # kde=True adds a smooth curve\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Values in df5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6ba2f-8455-41c3-8eea-51b44f4be78a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6f279-96bc-4232-9656-249082a1f486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
